{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "668d57d1",
   "metadata": {},
   "source": [
    "# 模型构建原理\n",
    "人工智能的第三次浪潮受益于卷积神经网络的出现和BP反向传播算法的实现，随着深度学习的发展，研究人员研究出了许许多多的模型，PyTorch中神经网络构造一般是基于 Module 类的模型来完成的，它让模型构造更加灵活。\n",
    "- PyTorch中神经网络的构造方法\n",
    "\n",
    "- PyTorch中特殊层的构建\n",
    "\n",
    "- LeNet的PyTorch实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e71b79",
   "metadata": {},
   "source": [
    "## 神经网络的构造\n",
    "Module 类是 nn 模块里提供的一个模型构造类，是所有神经⽹网络模块的基类，我们可以继承它来定义我们想要的模型。下面继承 Module 类构造多层感知机。这里定义的 MLP 类重载了 Module 类的 init 函数和 forward 函数。它们分别用于创建模型参数和定义前向计算。前向计算也即正向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc7927e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T10:14:31.767136Z",
     "start_time": "2022-09-04T10:14:31.752952Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "class MLP(nn.Module):\n",
    "    # 声明带有模型参数的层，这里声明了两个全连接层\n",
    "    def __init__(self, **kwargs):\n",
    "    # 调用MLP父类Block的构造函数来进行必要的初始化。这样在构造实例时还可以指定其他函数\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "        # 隐藏层\n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        self.act = nn.ReLU()\n",
    "        # 输出层\n",
    "        self.output = nn.Linear(256,10)\n",
    "    \n",
    "   # 定义模型的前向计算，即如何根据输入x计算返回所需要的模型输出\n",
    "    def forward(self, x):\n",
    "        o = self.act(self.hidden(x))\n",
    "        return self.output(o)   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43418817",
   "metadata": {},
   "source": [
    "我们可以实例化 MLP 类得到模型变量 net 。下⾯的代码初始化 net 并传入输⼊数据 X 做一次前向计算。其中， net(X) 会调用 MLP 继承⾃自 Module 类的 call 函数，这个函数将调⽤用 MLP 类定义的forward 函数来完成前向计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd3225b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T10:13:58.090105Z",
     "start_time": "2022-09-04T10:13:58.041522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (act): ReLU()\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1098, -0.0582,  0.1255, -0.1723, -0.2064, -0.0069,  0.0232, -0.1291,\n",
       "          0.1286, -0.2505],\n",
       "        [ 0.0169, -0.1222,  0.0599, -0.1869, -0.1833, -0.0387, -0.1653, -0.1275,\n",
       "          0.0816, -0.0743]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(2,784)\n",
    "net = MLP()\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaacb2ec",
   "metadata": {},
   "source": [
    "## 神经网络中常见的层\n",
    "深度学习的一个魅力在于神经网络中各式各样的层，例如全连接层、卷积层、池化层与循环层等等。虽然PyTorch提供了⼤量常用的层，但有时候我们依然希望⾃定义层。这里我们会介绍如何使用 Module 来自定义层，从而可以被反复调用。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81bde4",
   "metadata": {},
   "source": [
    "含模型参数的层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f72544d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T10:17:23.453878Z",
     "start_time": "2022-09-04T10:17:23.444867Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "    def forward(self, x):\n",
    "        return x - x.mean()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42adfcc3",
   "metadata": {},
   "source": [
    "测试，实例化该层，然后做前向计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adc9cc67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T10:18:58.292636Z",
     "start_time": "2022-09-04T10:18:58.271204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2., -1.,  0.,  1.,  2.])\n",
      "tensor([-2., -1.,  0.,  1.,  2.])\n"
     ]
    }
   ],
   "source": [
    "layer = MyLayer()\n",
    "# 1\n",
    "print(layer(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float)))\n",
    "# 2\n",
    "print(layer.forward(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0ecc16",
   "metadata": {},
   "source": [
    "不含模型参数的层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9061d3cd",
   "metadata": {},
   "source": [
    "我们还可以自定义含模型参数的自定义层。其中的模型参数可以通过训练学出。\n",
    "\n",
    "Parameter 类其实是 Tensor 的子类，如果一 个 Tensor 是 Parameter ，那么它会⾃动被添加到模型的参数列表里。所以在⾃定义含模型参数的层时，我们应该将参数定义成 Parameter ，除了直接定义成 Parameter 类外，还可以使⽤ ParameterList 和 ParameterDict 分别定义参数的列表和字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a4055b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T10:21:26.154266Z",
     "start_time": "2022-09-04T10:21:26.143652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyListDense(\n",
      "  (params): ParameterList(\n",
      "      (0): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (2): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (3): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyListDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyListDense, self).__init__()\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])\n",
    "        self.params.append(nn.Parameter(torch.randn(4, 1)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.params)):\n",
    "            x = torch.mm(x, self.params[i])\n",
    "        return x\n",
    "net = MyListDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35a96586",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T10:21:45.750688Z",
     "start_time": "2022-09-04T10:21:45.727322Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyDictDense(\n",
      "  (params): ParameterDict(\n",
      "      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]\n",
      "      (linear2): Parameter containing: [torch.FloatTensor of size 4x1]\n",
      "      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyDictDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDictDense, self).__init__()\n",
    "        self.params = nn.ParameterDict({\n",
    "                'linear1': nn.Parameter(torch.randn(4, 4)),\n",
    "                'linear2': nn.Parameter(torch.randn(4, 1))\n",
    "        })\n",
    "        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # 新增\n",
    "\n",
    "    def forward(self, x, choice='linear1'):\n",
    "        return torch.mm(x, self.params[choice])\n",
    "\n",
    "net = MyDictDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16af9951",
   "metadata": {},
   "source": [
    "二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。卷积层的模型参数包括了卷积核和标量偏差。在训练模型的时候，通常我们先对卷积核随机初始化，然后不断迭代卷积核和偏差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "036496e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T10:37:20.393278Z",
     "start_time": "2022-09-04T10:37:20.376253Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 卷积运算（二维互相关）\n",
    "def corr2d(X, K): \n",
    "    h, w = K.shape\n",
    "    X, K = X.float(), K.float()\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i: i + h, j: j + w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "# 二维卷积层\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd7ea3c",
   "metadata": {},
   "source": [
    "卷积窗口形状为p*q的卷积层称为p*q卷积层。\n",
    "\n",
    "填充(padding)是指在输⼊高和宽的两侧填充元素(通常是0元素)。\n",
    "\n",
    "下面的例子里我们创建一个⾼和宽为3的二维卷积层，然后设输⼊高和宽两侧的填充数分别为1。给定一 个高和宽为8的输入，我们发现输出的高和宽也是8。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27b93c98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T10:49:31.623881Z",
     "start_time": "2022-09-04T10:49:31.598812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# 定义一个函数来计算卷积层。它对输入和输出做相应的升维和降维\n",
    "# conv2d形式参数\n",
    "def comp_conv2d(conv2d,X):\n",
    "    # (1, 1)代表批量大小和通道数\n",
    "    X = X.view((1, 1) + X.shape)\n",
    "    Y = conv2d(X)\n",
    "    return Y.view(Y.shape[2:]) # 排除不关心的前两维:批量和通道\n",
    "\n",
    "# 注意这里是两侧分别填充1⾏或列，所以在两侧一共填充2⾏或列\n",
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3,padding=1)\n",
    "\n",
    "X = torch.rand(8, 8)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae74bdd7",
   "metadata": {},
   "source": [
    "当卷积核的高和宽不同时，我们也可以通过设置高和宽上不同的填充数使输出和输入具有相同的高和宽。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "270b2a34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T10:50:17.781055Z",
     "start_time": "2022-09-04T10:50:17.759990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用高为5、宽为3的卷积核。在⾼和宽两侧的填充数分别为2和1\n",
    "conv2d = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(5, 3), padding=(2, 1))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90d057e",
   "metadata": {},
   "source": [
    "在二维互相关运算中，卷积窗口从输入数组的最左上方开始，按从左往右、从上往下 的顺序，依次在输⼊数组上滑动。我们将每次滑动的行数和列数称为步幅(stride)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a35f5b23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T10:51:51.948722Z",
     "start_time": "2022-09-04T10:51:51.931198Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.Conv2d(1, 1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9268ca0c",
   "metadata": {},
   "source": [
    "填充可以增加输出的高和宽。这常用来使输出与输入具有相同的高和宽。\n",
    "\n",
    "步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的 ( 为大于1的整数)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031ccce",
   "metadata": {},
   "source": [
    "#### 池化层\n",
    "池化层每次对输入数据的一个固定形状窗口(⼜称池化窗口)中的元素计算输出。不同于卷积层里计算输⼊和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也 分别叫做最大池化或平均池化。在二维最⼤池化中，池化窗口从输入数组的最左上方开始，按从左往右、从上往下的顺序，依次在输⼊数组上滑动。当池化窗口滑动到某⼀位置时，窗口中的输入子数组的最大值即输出数组中相应位置的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b736e4c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T13:38:18.660351Z",
     "start_time": "2022-09-04T13:38:18.646359Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96b90577",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T13:38:33.866348Z",
     "start_time": "2022-09-04T13:38:33.858358Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]], dtype=torch.float)\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03a686dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T13:38:42.533646Z",
     "start_time": "2022-09-04T13:38:42.521602Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X, (2, 2), 'avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fc7223",
   "metadata": {},
   "source": [
    "### 模型示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36805fe",
   "metadata": {},
   "source": [
    "- LeNet  \n",
    "这是一个简单的前馈神经网络 (feed-forward network）（LeNet）。它接受一个输入，然后将它送入下一层，一层接一层的传递，最后给出输出。\n",
    "\n",
    "一个神经网络的典型训练过程如下：\n",
    "\n",
    "1. 定义包含一些可学习参数(或者叫权重）的神经网络\n",
    "\n",
    "2. 在输入数据集上迭代\n",
    "\n",
    "3. 通过网络处理输入\n",
    "\n",
    "4. 计算 loss (输出和正确答案的距离）\n",
    "\n",
    "5. 将梯度反向传播给网络的参数\n",
    "\n",
    "更新网络的权重，一般使用一个简单的规则：weight = weight - learning_rate * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44078297",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T13:47:39.911114Z",
     "start_time": "2022-09-04T13:47:39.902096Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 输入图像channel：1；输出channel：6；5x5卷积核\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 2x2 Max pooling\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # 如果是方阵,则可以只使用一个数字进行定义\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # 除去批处理维度的其他所有维度\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e44771",
   "metadata": {},
   "source": [
    "我们只需要定义 forward 函数，backward函数会在使用autograd时自动定义，backward函数用来计算导数。我们可以在 forward 函数中使用任何针对张量的操作和计算。\n",
    "\n",
    "一个模型的可学习参数可以通过net.parameters()返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9212d17c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T13:48:20.779519Z",
     "start_time": "2022-09-04T13:48:20.774524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1的权重 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de47f494",
   "metadata": {},
   "source": [
    "让我们尝试一个随机的 32x32 的输入。注意:这个网络 (LeNet）的期待输入是 32x32 的张量。如果使用 MNIST 数据集来训练这个网络，要把图片大小重新调整到 32x32。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2e17175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T13:48:53.401479Z",
     "start_time": "2022-09-04T13:48:53.391972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0289, -0.0138,  0.0575,  0.0399, -0.0422,  0.0412, -0.1654, -0.0204,\n",
      "          0.0178, -0.0687]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6f004",
   "metadata": {},
   "source": [
    "清零所有参数的梯度缓存，然后进行随机梯度的反向传播："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "134f634a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T13:49:11.107220Z",
     "start_time": "2022-09-04T13:49:11.089223Z"
    }
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0d61f1",
   "metadata": {},
   "source": [
    "注意：torch.nn只支持小批量处理 (mini-batches）。整个 torch.nn 包只支持小批量样本的输入，不支持单个样本的输入。比如，nn.Conv2d 接受一个4维的张量，即nSamples x nChannels x Height x Width 如果是一个单独的样本，只需要使用input.unsqueeze(0) 来添加一个“假的”批大小维度。\n",
    "\n",
    "- torch.Tensor - 一个多维数组，支持诸如backward()等的自动求导操作，同时也保存了张量的梯度。\n",
    "\n",
    "- nn.Module - 神经网络模块。是一种方便封装参数的方式，具有将参数移动到GPU、导出、加载等功能。\n",
    "\n",
    "- nn.Parameter - 张量的一种，当它作为一个属性分配给一个Module时，它会被自动注册为一个参数。\n",
    "\n",
    "- autograd.Function - 实现了自动求导前向和反向传播的定义，每个Tensor至少创建一个Function节点，该节点连接到创建Tensor的函数并对其历史进行编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c056ef3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
