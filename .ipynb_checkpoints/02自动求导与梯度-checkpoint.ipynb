{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "494a84d8",
   "metadata": {},
   "source": [
    "# 自动求导机制\n",
    "PyTorch 中，所有神经网络的核心是 autograd 包。autograd包为张量上的所有操作提供了自动求导机制。它是一个在运行时定义 ( define-by-run ）的框架，这意味着反向传播是根据代码如何运行来决定的，并且每次迭代可以是不同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67024fc",
   "metadata": {},
   "source": [
    "torch.Tensor 是这个包的核心类。如果设置它的属性 .requires_grad 为 True，那么它将会追踪对于该张量的所有操作。当完成计算后可以通过调用 .backward()，来自动计算所有的梯度。这个张量的所有梯度将会自动累加到.grad属性。\n",
    "\n",
    "注意：在 y.backward() 时，如果 y 是标量，则不需要为 backward() 传入任何参数；否则，需要传入一个与 y 同形的Tensor。\n",
    "\n",
    "要阻止一个张量被跟踪历史，可以调用.detach()方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。为了防止跟踪历史记录(和使用内存），可以将代码块包装在 with torch.no_grad(): 中。在评估模型时特别有用，因为模型可能具有 requires_grad = True 的可训练的参数，但是我们不需要在此过程中对他们进行梯度计算。\n",
    "\n",
    "还有一个类对于autograd的实现非常重要：Function。Tensor 和 Function 互相连接生成了一个无环图 (acyclic graph)，它编码了完整的计算历史。每个张量都有一个.grad_fn属性，该属性引用了创建 Tensor 自身的Function(除非这个张量是用户手动创建的，即这个张量的grad_fn是 None )。下面给出的例子中，张量由用户手动创建，因此grad_fn返回结果是None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "facfafb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T05:46:11.429579Z",
     "start_time": "2022-09-04T05:46:10.138785Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# 机制实现 \n",
    "from __future__ import print_function\n",
    "import torch\n",
    "x = torch.randn(3,3,requires_grad=True)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38027f18",
   "metadata": {},
   "source": [
    "如果需要计算导数，可以在 Tensor 上调用 .backward()。如果 Tensor 是一个标量(即它包含一个元素的数据），则不需要为 backward() 指定任何参数，但是如果它有更多的元素，则需要指定一个gradient参数，该参数是形状匹配的张量。\n",
    "\n",
    "创建一个张量并设置requires_grad=True用来追踪其计算历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "584aa0ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T06:13:09.186502Z",
     "start_time": "2022-09-04T06:13:09.164391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.ones(2,2,dtype=float,requires_grad=True);\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774cfed7",
   "metadata": {},
   "source": [
    "Do a calculation on this tensor,because tensor y is the request\n",
    "of the calculation,  so you can see the grad_fn have changed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b6c7b5b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T06:13:10.440292Z",
     "start_time": "2022-09-04T06:13:10.430772Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64, grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x ** 2;\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056135bd",
   "metadata": {},
   "source": [
    "Do more on tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "407a0d87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T06:13:13.563842Z",
     "start_time": "2022-09-04T06:13:13.541763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], dtype=torch.float64, grad_fn=<MulBackward0>) tensor(3., dtype=torch.float64, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713efe20",
   "metadata": {},
   "source": [
    "### 原地修改\n",
    ".requires_grad_(...) 原地改变了现有张量的requires_grad标志。如果没有指定的话，默认输入的这个标志是 False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a89b808",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T06:13:15.910148Z",
     "start_time": "2022-09-04T06:13:15.886100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x0000025B6E149C70>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2) # 缺失情况下默认 requires_grad = False\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799c3f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T06:02:21.538374Z",
     "start_time": "2022-09-04T06:02:21.532303Z"
    }
   },
   "source": [
    "## 梯度\n",
    "反向传播你可以将其理解为求导的过程！，现在开始进行反向传播，因为 out 是一个标量，因此out.backward()和 out.backward(torch.tensor(1.)) 等价。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4235d39c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T06:13:16.710502Z",
     "start_time": "2022-09-04T06:13:16.693962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(out)\n",
    "# 反向传播\n",
    "out.backward(retain_graph=True)# 标量的反向传播直接来就行\n",
    "# 输出导数 d(out)/dx\n",
    "# ！！wr\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d4ba12a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T06:33:03.028332Z",
     "start_time": "2022-09-04T06:33:02.997261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out >>>  tensor(3., dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "x >>>  tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64, requires_grad=True)\n",
      "out2 >>>  tensor(4., dtype=torch.float64, grad_fn=<SumBackward0>)\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]], dtype=torch.float64)\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(\"out >>> \",out)\n",
    "print(\"x >>> \",x)\n",
    "# 再来反向传播⼀一次，注意grad是累加的\n",
    "out2 = x.sum()\n",
    "print(\"out2 >>> \",out2)\n",
    "\n",
    "# 求导\n",
    "out2.backward()\n",
    "# 为什么用x.grad是因为 后续都是以x为自变量导出的！\n",
    "print(x.grad)\n",
    "\n",
    "\n",
    "# 注意：grad在反向传播过程中是累加的(accumulated)，\n",
    "# 这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。\n",
    "out3 = x.sum()\n",
    "x.grad.data.zero_()\n",
    "out3.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15cfcee",
   "metadata": {},
   "source": [
    "`现在我们来看一个雅可比向量积的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0cf4f49a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T06:49:41.604238Z",
     "start_time": "2022-09-04T06:49:41.576230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2408, -0.0160, -1.3541], requires_grad=True)\n",
      "tensor(7.3468)\n",
      "tensor(14.6935)\n",
      "tensor(29.3870)\n",
      "tensor(58.7741)\n",
      "tensor(117.5482)\n",
      "tensor(235.0963)\n",
      "tensor(470.1926)\n",
      "tensor(940.3853)\n",
      "tensor(1880.7705)\n",
      "y:  tensor([-1270.6161,   -16.3425, -1386.5660], grad_fn=<MulBackward0>)\n",
      "i:  9\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x * 2\n",
    "i = 0\n",
    "\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "    i = i + 1\n",
    "    print(y.data.norm())\n",
    "print(\"y: \",y)\n",
    "print(\"i: \",i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae6c69",
   "metadata": {},
   "source": [
    "在这种情况下，y 不再是标量。torch.autograd 不能直接计算完整的雅可比矩阵，但是如果我们只想要雅可比向量积，只需将这个向量作为参数传给 backward："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e24df20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T06:49:45.814716Z",
     "start_time": "2022-09-04T06:49:45.803204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1270.6161,   -16.3425, -1386.5660], grad_fn=<MulBackward0>)\n",
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3bb239",
   "metadata": {},
   "source": [
    "也可以通过将代码块包装在 with torch.no_grad(): 中，来阻止 autograd 跟踪设置了.requires_grad=True的张量的历史记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "abb16d56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T06:54:09.538919Z",
     "start_time": "2022-09-04T06:54:09.530915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "# 取消自动求导\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc97fe2",
   "metadata": {},
   "source": [
    "如果我们想要修改 tensor 的数值，但是又不希望被 autograd 记录(即不会影响反向传播)， 那么我们可以对 tensor.data 进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e69a7bee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T06:56:33.565775Z",
     "start_time": "2022-09-04T06:56:33.542706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.])\n",
      "False\n",
      "tensor([100.], requires_grad=True)\n",
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(1,requires_grad=True)\n",
    "\n",
    "print(x.data) # 还是一个tensor\n",
    "print(x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "\n",
    "y.backward()\n",
    "print(x) # 更改data的值也会影响tensor的值 \n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45709c00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-04T06:59:38.534483Z",
     "start_time": "2022-09-04T06:59:38.521965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "x = torch.tensor([3.0],requires_grad=True)\n",
    "# 函数是这样的\n",
    "y = 2 * x\n",
    "# 对x求导\n",
    "y.backward();\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3ce598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c009130b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e2330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
